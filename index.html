<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Abhinav Narayan Harish</title>
  
  <meta name="author" content="Abhinav Harish">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="kk_oB9YWBIQJXAP8_h68BzBQgJOd0tL-dK5yfSnu5eU" />
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                    <tr style="padding:0px">
                        <td style="padding:2.5%;width:63%;vertical-align:middle">
                            <p style="text-align:center">
                                <name>Abhinav Narayan Harish</name>
                            </p>
                            <p>I am a CS PhD student at University of Wisconsin Madison working with Professor Josiah Hanna. Prior to this, I was a Master's student with Professor Zsolt Kira working on using Reinforcement Learning (RL) for long-horizon home-rearrangement in Habitat2.0. I received my Bachelor's degree in Electrical Engineering with a minor in Computer Science from IITGN .</p> 
                            <p>
                                <!--At Google I've worked on <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I did my bachelors at the <a href="http://cs.toronto.edu">University of Toronto</a>.
                        I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.-->
                            </p>
                            <p style="text-align:center">
                                <a href="https://drive.google.com/file/d/1AhiVddxPEt_xo-0DV4Xukp4qv0B5Te8-/view?usp=sharing">CV</a>&nbsp/&nbsp
                                <a href="mailto:abhinavn.harish@gmail.com">Email</a> &nbsp/&nbsp
                                <!-- <a href="data/aditya_vora_cv.pdf">CV</a> &nbsp/&nbsp -->
                                <a href="https://scholar.google.com/citations?user=pw-CSfoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                                <a href="https://www.linkedin.com/in/abhinav-harish-825336129/">LinkedIn</a> &nbsp/&nbsp
                                <a href="https://github.com/absdnd">GitHub</a> &nbsp/&nbsp
                                <a href="https://twitter.com/Abhinav39469942">Twitter</a>
                            </p>
                        </td>
                        <td style="padding:2.5%;width:40%;max-width:40%">
                            <a href="images/abhinav_circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/abhinav_circle.png" class="hoverZoomLink"></a>
                        </td>
                    </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                    <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <heading>Research</heading>
                            <p>
                                Broadly, I'm interested in Reinforcement Learning (RL) and it's application in the field of Robotics. My goal is to explore the limits of generalization in long-horizon RL across tasks and applied to distinct environments. To this end, I have explored hierarchical RL using Skill-Chaining for my Master's thesis. With a background in 3D Computer Vision, I'm interested in leveraging this to bridging the Sim2Real gap, enabling seamless interaction in the real world.   
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>

            <!--preprints-->
            <!--  <table style="width:100%;border:0px;border-spacing:0px;
    border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:20px">
        <tbody>
        <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
                <heading>(Pr)ePrints</heading>
            </td>
        </tr>
        </tbody>
    </table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
    margin-right:auto;margin-left:auto;">
        <tbody>

        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/aditya-fchd.png' width="250" height="100"></div>
            </td>
            <td width="75%" valign="middle">
                <p>
                    <a href="https://arxiv.org/abs/1809.08766">
                        <papertitle>FCHD: Fast and accurate head detection in crowded scenes</papertitle>
                    </a>
                    <br>
                    <strong>Aditya Vora</strong>,
                    Vinay Chilaka
                    <br>
                    <a href="https://arxiv.org/abs/1809.08766">arxiv</a> /
                    <a href="data/aditya-fchd.bib">bibtex</a> /
                    <a href="https://github.com/aditya-vora/FCHD-Fully-Convolutional-Head-Detector">code</a>
                </p>
                <p></p>
                <p>
                    A fully convolutional single stage head detector is proposed where the anchor scales are designed by taking effective receptive field into account, hence giving better average precision especially for small heads in crowded scenes.
                </p>
            </td>
        </tr>

        </tbody>
    </table> -->
            <!-- Publication -->
            <table style="width:100%;border:0px;border-spacing:0px;
        border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:20px">
                <tbody>
                    <tr>
                        <td style="padding:10px;width:100%;vertical-align:middle">
                            <heading>Publications</heading>
                        </td>
                    </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
                <tbody>

                    <tr>
                        <td style="padding:20px;width:35%;vertical-align:middle">
                            <img src='images/aux_distill.png' width="250" height="120"></div>
                        </td>
                        <td width="75%" valign="middle">
                            <p>
                                <!-- <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865518300473"> -->
                                <papertitle>Reinforcement Learning via Auxiliary Task Distillation</papertitle>
                                </a>
                                <br>
                                <strong>Abhinav Narayan Harish</strong>, Larry Heck, Josiah Hanna, Zsolt Kira, Andrew Szot
                                <br>
                                <em>
                                    European Conference of Computer Vision, 2024
                                </em>
                                <br>
                                <a href="https://arxiv.org/abs/2406.17168">pdf</a> / <a href="https://github.com/absdnd/aux_distill">code</a> 
                            </p>
                            <p></p>
                            <p>
                                We present Reinforcement Learning via Auxiliary Task Distillation (AuxDistill), a new method that enables reinforcement learning (RL) to perform long-horizon robot control problems by distilling behaviors from auxiliary RL tasks. AuxDistill achieves this by concurrently carrying out multi-task RL with auxiliary tasks, which are easier to learn and relevant to the main task. A weighted distillation loss transfers behaviors from these auxiliary tasks to solve the main task. We demonstrate that AuxDistill can learn a pixels-to-actions policy for a challenging multi-stage embodied object rearrangement task from the environment reward without demonstrations, a learning curriculum, or pre-trained skills. AuxDistill achieves 2.3x higher success than the previous state-of-the-art baseline in the Habitat Object Rearrangement benchmark and outperforms methods that use pre-trained skills and expert demonstrations.                            
                            </p>
                        </td>
                    </tr>

                </tbody>
            </table>
            
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
                <tbody>

                    <tr>
                        <td style="padding:20px;width:35%;vertical-align:middle">
                            <img src='images/handoff_issue.png' width="250" height="150"></div>
                        </td>
                        <td width="75%" valign="middle">
                            <p>
                                <!-- <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865518300473"> -->
                                <papertitle>Skill-Chaining for Long-Horizon Rearrangement</papertitle>
                                </a>
                                <br>
                                <strong>Abhinav Narayan Harish</strong>
                                <br>
                                <em>
                                    Master's thesis, Georgia Institute of Technology
                                </em>
                                <br>
                                <a href="https://drive.google.com/file/d/1c54ABpfoJac1jz51-NISTnC-34thE5nj/view?usp=sharing">pdf</a> /
                            </p>
                            <p></p>
                            <p>
                                Given a set of pre-trained skills performing specific aspects of rearrangement , i.e Navigating, Picking up or Placing objects, we developed a hierarchical fine-tuning scheme that can fine-tune these policies simultaneously. Our method addresses the hand-off challenge in rearrangement where subsequent skills are not aligned (i.e the navigation skill-terminates too far from the couch). Our method demonstrates superior performance (about 13% in rearrangement success) over a static policy. 
                            </p>
                        </td>
                    </tr>

                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
                <tbody>

                    <tr>
                        <td style="padding:20px;width:35%;vertical-align:middle">
                            <img src='images/RGL_teaser.gif' width="250" height="150"></div>
                        </td>
                        <td width="75%" valign="middle">
                            <p>
                                <!-- <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865518300473"> -->
                                <papertitle>RGL-NET: A Recurrent Graph Learning Framework for Progressive Part Assembly</papertitle>
                                </a>
                                <br>
                                <strong>Abhinav Narayan Harish</strong>, Rajendra Nagar,
                                <a href="https://people.iitgn.ac.in/~shanmuga/">Shanmuganathan Raman</a>
                                <br>
                                <em>
                                    Winter Conference on Applications of Computer Vision <a href="http://wacv2022.thecvf.com/home">
                                        (WACV 2022)
                                    </a>
                                </em>
                                <br>
                                <a href="https://arxiv.org/abs/2107.12859">pdf</a> /
                                <a href="data/abhinav-rgl-net.bib">bibtex</a>
                            </p>
                            <p></p>
                            <p>
                                We propose an assembly framework that can assemble a shape in a canonical order by progressively gathering information. Compared to prior frameworks, our method achieves upto 10% improvement in part accuracy and 15% improvement in connectivity accuracy.
                            </p>
                        </td>
                    </tr>

                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
                <tbody>

                    <tr>
                        <td style="padding:20px;width:35%;vertical-align:middle">
                            <img src='images/MLSP_result.png' width="250" height="120"></div>
                        </td>
                        <td width="75%" valign="middle">
                            <p>
                               
                                <papertitle>Double Compression Detection of Distinguishable blocks in JPEG images compressed with the same quantization matrix.</papertitle>
                                <br>
                                <strong>Abhinav Narayan Harish*</strong>, Vinay Verma*,
                                <a href="https://www.iitbhilai.ac.in/index.php?pid=nitin">Nitin Khanna</a>
                                <br>
                                <em>
                                    International Workshop on Machine Learning for Signal Processing <a href="https://ieeemlsp.cc/">
                                        (MLSP)
                                    </a>,
                                </em> 2020
                                <br>
                                <a href="https://ieeexplore.ieee.org/document/9231749">pdf</a> /
                                <a href="data/abhinav-double-comp.bib">bibtex</a>
                            </p>
                            <p></p>
                            <p>
                                In this paper, we propose a deep learning based approach to localize forgery in a JPEG image. We develop a multi-coloumn CNN architecture that utilizes spatial and frequency domain information and classify each 8x8 JPEG block as single or double compressed.
                            </p>
                        </td>
                    </tr>

                </tbody>
            </table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
                <tbody>

                    <tr>
                        <td style="padding:20px;width:35%;vertical-align:middle">
                            <img src='images/double_comp_SPIN.png' width="250" height="120"></div>
                        </td>
                        <td width="75%" valign="middle">
                            <p>
                                <papertitle>Neural Network based Block-Level Detection of Same Quality Factor Double JPEG Compression</papertitle>
                                    <br>
                                    A.Deshpande*, <strong>Abhinav Narayan Harish*</strong>, S.Singh*, V. Verma,
                                    <a href="https://www.iitbhilai.ac.in/index.php?pid=nitin">Nitin Khanna</a>
                                    <br>
                                    <em>
                                        International Conference on Signal Processing and Integrated Networks <a href="https://www.amity.edu/spin2020/">
                                            (SPIN)
                                        </a>,
                                    </em> 2020
                                    <br>
                                    <a href="https://ieeexplore.ieee.org/document/9070977">pdf</a> /
                                    <a href="data/abhinav-nn-double-comp.bib">bibtex</a>
</p>
                            <p></p>
                            <p>
                                We detect double compression at the patch level (64 x 64) or (128 x 128) by introducing an additional feature based on difference of DCT coefficients. Our classification network achieves upto 1.52% improvement in detection accuracy.
                            </p>
                        </td>
                    </tr>

                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;
        border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:20px">
                <tbody>
                    <tr>
                        <td style="padding:10px;width:100%;vertical-align:middle">
                            <heading>Projects</heading>

                        </td>
                    </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
                <tbody>

                    <tr>
                        <td style="padding:20px;width:35%;vertical-align:middle">
                            <img src='images/LangShape.png' width="250" height="150"></div>
                        </td>
                        <td width="75%" valign="middle">
                            <p>
                                <papertitle>Language as a Means of Shape Differentiations</papertitle>
                                </a>
                                <br>
                                <em>
                                    Statistical Machine Learning, (ECE-6254) Georgia Tech
                                    </a>
                                </em>
                                <br>
                                <a href="https://www.dropbox.com/s/gt67p9kf2xaragc/StatML_FinalProject_Report.pdf?dl=0">pdf </a>
                            </p>
                            <p></p>
                            <p>
                                We utilize language tokens to understand distinctions between shapes. Given a text-label we find the closest match to the target shape.
                            </p>
                        </td>
                    </tr>

                </tbody>
            </table>

            <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
                <tbody>

                    <tr>
                        <td style="padding:20px;width:35%;vertical-align:middle">
                            <img src='images/LinUCB_evenly_sampled_arms_updated.png' width="250" height="150"></div>
                        </td>
                        <td width="75%" valign="middle">
                            <p>
                                <papertitle>Inverse Reinforcement Learning on Multi-Armed Bandits</papertitle>
                                </a>
                                <br>
                                <em>
                                    Online Decision Making in Machine Learning (ECE-8803), Georgia Tech
                                    </a>
                                </em>
                                <br>
                                <a href="https://www.dropbox.com/s/w4e1qnvkw24d3nd/ODML_Project_Report.pdf?dl=0">pdf</a>
                            </p>
                            <p></p>
                            <p>
                                We study the problem of Inverse Reinforcement Learning on the LinUCB algorithm and obtaining an optimal regret gurantee.
                            </p>
                        </td>
                    </tr>

                </tbody>
            </table> -->

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
                <tbody>

                    <tr>
                        <td style="padding:20px;width:35%;vertical-align:middle">
                            <img src='images/EMM.gif' width="250" height="150"></div>
                        </td>
                        <td width="75%" valign="middle">
                            <p>
                                <papertitle>Eulerian Motion Magnification</papertitle>
                                </a>
                                <br>
                                <em>
                                    3D Computer Vision, IIT Gandhinagar
                                    </a>
                                </em>
                                <br>
                                <a href="https://github.com/absdnd/Eulerian_Motion_Magnification">code</a>
                            </p>
                            <p></p>
                            <p>
                                We develop a python implementation of the <a href="http://people.csail.mit.edu/mrub/evm/">Eulerian Motion Magnification</a> algorithm for revealing subtle changes in the world.
                            </p>
                        </td>
                    </tr>

                </tbody>
            </table>

            <!-- <table style="width:100%;border:0px;border-spacing:0px;
        border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:20px">
                <tbody>
                    <tr>
                        <td style="padding:10px;width:100%;vertical-align:middle">
                            <heading>Animations</heading>

                        </td>
                    </tr>
                </tbody>
            </table> -->

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
                <tbody>

                    <tr>
                        <td style="padding:20px;width:35%;vertical-align:middle">
                            <img src='data/buzzyBowlGIF.gif' width="250" height="150"></div>
                        </td>
                        <td width="75%" valign="middle">
                            <p>
                                <papertitle>OpenGL Buzzy's Bowl</papertitle>
                                </a>
                                <br>
                                <em>
                                    Advanced Programming Techniques (ECE-6122), Georgia Tech
                                    </a>
                                </em>
                                <br>
                            </p>
                            <p></p>
                            <p>
                                Simulation of the launching of UAV's from a football field and interacting with each other as the rotate on a sphere. Implemented using Open-GL using C++ 
                            </p>
                        </td>
                    </tr>

                </tbody>
            </table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
        margin-right:auto;margin-left:auto;">
                <tbody>

                    <tr>
                        <td style="padding:20px;width:35%;vertical-align:middle">
                            <img src='data/demo.gif' width="250" height="180"></div>
                        </td>
                        <td width="75%" valign="middle">
                            <p>
                                <papertitle>Interactive Interpolation between Splines</papertitle>
                                </a>
                                <br>
                                <em>
                                    Computer Animation (CS-7496) Georgia Tech
                                    </a>
                                </em>
                                <br>
                                <a href="https://github.com/absdnd/Interactive_Interpolation">code</a>
                            </p>
                            <p></p>
                            <p>
                                An interactive tool for visualization for visualizing different types of spline interpolation between two points. Implemented using Python using Qt5 library.  
                            </p>
                        </td>
                    </tr>

                </tbody>
            </table>


            <!-- Bottom -->
            <table style="width:100%;vertical-align:center;border:0px;border-spacing:0px;padding:0px">
                <tr>
                    <td>
                        <br>
                        <!-- <p style="text-align:right;font-size:small;margin-bottom: 0">
                    <a href="https://cseweb.ucsd.edu/~kriegman/">&#10025;</a>
                    <a href="http://pages.ucsd.edu/~ztu/">&#10025;</a>
                    <a href="http://ai.stanford.edu/~ssrinath">&#10025;</a>
                    <a href="https://chenshen.xyz">&#10025;</a>
                    <a href="https://orzyt.cn">&#10025;</a>
                    <a href="http://floatingsong.com/">&#10025;</a>
                </p> -->
                        <hr style="margin-bottom:0;margin-top: 0">
                        <p style="text-align:left;font-size:10px">
                            <span style="font-size:10px;float:right">
                                Website inspired by <a href="https://jonbarron.info/" style="font-size: 10px;">Jon Barron</a>
                            </span>
                        </p>
                    </td>
                </tr>
            </table>

        </td>
    </tr>
  </table>
</body>

</html>
