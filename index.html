<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Abhinav Narayan Harish</title>
  
  <meta name="author" content="Abhinav Harish">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="kk_oB9YWBIQJXAP8_h68BzBQgJOd0tL-dK5yfSnu5eU" />
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                    <tr style="padding:0px">
                        <td style="padding:2.5%;width:63%;vertical-align:middle">
                            <p style="text-align:center">
                                <name>Abhinav Narayan Harish</name>
                            </p>
                            </p style="text-align:center">
                            </p>
                            <p>
                                I am a CS PhD student at University of Wisconsin Madison working with Professor Josiah Hanna. Prior to this, I was a Master's student with Professor Zsolt Kira working on using Reinforcement Learning (RL) for long-horizon rearrangement. 
                                I received my Bachelor's degree in Electrical Engineering with a minor in Computer Science from IITGN .</p> 
                            <p>

                            <p>
                                Broadly, I'm interested in Reinforcement Learning (RL) and it's application in the field of Robotics. My goal is to explore using end to end RL to learn long horizon tasks in simulation, and transfer them to the real world.
                            </p>

                                <!--At Google I've worked on <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I did my bachelors at the <a href="http://cs.toronto.edu">University of Toronto</a>.
                        I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.-->
                            </p>
                            <p style="text-align:center">
                                <a href="https://drive.google.com/file/d/1El1ssHiwfw4bt-J0asL_VxF9KKgZPd9X/view?usp=sharing">CV</a>&nbsp/&nbsp
                                <!-- <a href="data/aditya_vora_cv.pdf">CV</a> &nbsp/&nbsp -->
                                <a href="https://scholar.google.com/citations?user=pw-CSfoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                                <a href="https://www.linkedin.com/in/abhinav-harish-825336129/">LinkedIn</a> &nbsp/&nbsp
                                <a href="https://github.com/absdnd">GitHub</a> 
                            </p>
                        </td>
                        <td style="padding:2.5%;width:40%;max-width:40%">
                            <a href="images/abhinav_photo.jfif"><img style="width:100%;max-width:100%" alt="profile photo" src="images/abhinav_photo.jfif" class="hoverZoomLink"></a>
                        </td>
                    </tr>
                </tbody>
            </table>
            <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                    <tr>
                        <td style="padding:20px;width:100%;vertical-align:middle">
                            <heading>Research</heading>
                            <p>
                                Broadly, I'm interested in Reinforcement Learning (RL) and it's application in the field of Robotics. My goal is to explore the limits of generalization in long-horizon RL across tasks and applied to distinct environments. To this end, I have explored hierarchical RL using Skill-Chaining for my Master's thesis. With a background in 3D Computer Vision, I'm interested in leveraging this to bridging the Sim2Real gap, enabling seamless interaction in the real world.   
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table> -->

            <!--preprints-->
            <!--  <table style="width:100%;border:0px;border-spacing:0px;
    border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:20px">
        <tbody>
        <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
                <heading>(Pr)ePrints</heading>
            </td>
        </tr>
        </tbody>
    </table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
    margin-right:auto;margin-left:auto;">
        <tbody>

        <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/aditya-fchd.png' width="250" height="100"></div>
            </td>
            <td width="75%" valign="middle">
                <p>
                    <a href="https://arxiv.org/abs/1809.08766">
                        <papertitle>FCHD: Fast and accurate head detection in crowded scenes</papertitle>
                    </a>
                    <br>
                    <strong>Aditya Vora</strong>,
                    Vinay Chilaka
                    <br>
                    <a href="https://arxiv.org/abs/1809.08766">arxiv</a> /
                    <a href="data/aditya-fchd.bib">bibtex</a> /
                    <a href="https://github.com/aditya-vora/FCHD-Fully-Convolutional-Head-Detector">code</a>
                </p>
                <p></p>
                <p>
                    A fully convolutional single stage head detector is proposed where the anchor scales are designed by taking effective receptive field into account, hence giving better average precision especially for small heads in crowded scenes.
                </p>
            </td>
        </tr>

        </tbody>
    </table> -->
            <!-- Publication -->
               
            <table style="width:100%;border:0px;border-spacing:0px;
        border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:20px">
                <tbody>
                    <tr>
                        <td style="padding:10px;width:100%;vertical-align:middle">
                            <heading>Publications</heading>

                        </td>
                    </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate; margin-right:auto;margin-left:auto;">
                <tbody>
                    <tr>
                        <td style="padding:20px;width:40%;vertical-align:middle">
                            <img src='images/icra2025.png' width="250" height="120">
                        </td>
                        <td width="60%" valign="middle">
                            <p>
                                <papertitle>Reinforcement Learning Within the Classical Robotics Stack: A Case
                                    Study in Robot Soccer</papertitle>
                                <br>
                                Adam Labiosa, Zhihan Wang, Siddhant Agarwal, William Cong, Geethika Hemkumar,
                                <strong>Abhinav Narayan Harish</strong>, Benjamin Hong, Josh Kelle, Chen Li, Yuhao Li, Zisen Shao,
                                Peter Stone, Josiah P. Hanna
                                <br>
                                <em>ICRA 2025</em>
                                <br>
                                <a href="https://arxiv.org/pdf/2412.09417#:~:text=To%20address%20this%20challenge%20in%20the%20RoboCup%20Standard,decomposing%20behavior%20into%20learned%20sub-behaviors%20with%20heuristic%20selection.">pdf</a>
                            </p>
                            <!-- Right Facing Caret for toggling abstract with "Abstract" text -->
                            <span onclick="toggleAbstract('abstract1')" style="cursor: pointer; font-size: 10px; color: black; font-weight: bold;">
                                &#8594; <!-- Right Facing Caret -->
                                <span style="font-size: 10px; margin-left: 10px;">Abstract</span>
                            </span>
                            <div id="abstract1" style="display:none; margin-top: 10px;">
                                <p>
                                    Robot decision-making in partially observable, real-time, dynamic, and multi-agent environments remains a difficult and unsolved challenge. Model-free reinforcement learning (RL) is a promising approach to learning decision
                                    making in such domains, however, end-to-end RL in complex environments is often intractable. To address this challenge in the RoboCup Standard Platform League (SPL) domain, we developed a novel architecture integrating RL within a classical robotics stack, while employing a multi-fidelity sim2real
                                    approach and decomposing behavior into learned sub-behaviors with heuristic selection. Our architecture led to victory in the 2024 RoboCup SPL Challenge Shield Division. In this work, we fully describe our system’s architecture and empirically
                                    analyze key design decisions that contributed to its success. Our approach demonstrates how RL-based behaviors can be integrated into complete robot behavior architectures.
                                </p>
                            </div>
                        </td>
                    </tr>
                    <!-- First Content -->
                    <tr>
                        <td style="padding:20px;width:40%;vertical-align:middle">
                            <img src='images/aux_distill.png' width="250" height="120">
                        </td>
                        <td width="60%" valign="middle">
                            <p>
                                <papertitle>Reinforcement Learning via Auxiliary Task Distillation</papertitle>
                                <br>
                                <strong>Abhinav Narayan Harish</strong>, Larry Heck, Josiah Hanna, Zsolt Kira, Andrew Szot
                                <br>
                                <em>European Conference of Computer Vision, 2024</em>
                                <br>
                                <a href="https://arxiv.org/abs/2406.17168">pdf</a> / <a href="https://github.com/absdnd/aux_distill">code</a>
                            </p>
                            <!-- Right Facing Caret for toggling abstract with "Abstract" text -->
                            <span onclick="toggleAbstract('abstract1')" style="cursor: pointer; font-size: 10px; color: black; font-weight: bold;">
                                &#8594; <!-- Right Facing Caret -->
                                <span style="font-size: 10px; margin-left: 10px;">Abstract</span>
                            </span>
                            <div id="abstract1" style="display:none; margin-top: 10px;">
                                <p>
                                    We present Reinforcement Learning via Auxiliary Task Distillation (AuxDistill), a new method that enables reinforcement learning (RL) to perform long-horizon robot control problems by distilling behaviors from auxiliary RL tasks. AuxDistill achieves this by concurrently carrying out multi-task RL with auxiliary tasks, which are easier to learn and relevant to the main task. A weighted distillation loss transfers behaviors from these auxiliary tasks to solve the main task. We demonstrate that AuxDistill can learn a pixels-to-actions policy for a challenging multi-stage embodied object rearrangement task from the environment reward without demonstrations, a learning curriculum, or pre-trained skills. AuxDistill achieves 2.3x higher success than the previous state-of-the-art baseline in the Habitat Object Rearrangement benchmark and outperforms methods that use pre-trained skills and expert demonstrations.
                                </p>
                            </div>
                        </td>
                    </tr>
                    
                    <!-- Second Content -->
                    <tr>
                        <td style="padding:20px;width:40%;vertical-align:middle">
                            <img src='images/rearrangement.png' width="250" height="120">
                        </td>
                        <td width="60%" valign="middle">
                            <p>
                                <papertitle>Skill-Chaining for Long-Horizon Rearrangement</papertitle>
                                <br>
                                <strong>Abhinav Narayan Harish</strong>
                                <br>
                                <em>Master's Thesis, Georgia Institute of Technology</em>
                                <br>
                                <a href="https://drive.google.com/file/d/1c54ABpfoJac1jz51-NISTnC-34thE5nj/view?usp=sharing">pdf</a>
                            </p>
                            <!-- Right Facing Caret for toggling abstract with "Abstract" text -->
                            <span onclick="toggleAbstract('abstract2')" style="cursor: pointer; font-size: 10px; color: black; font-weight: bold;">
                                &#8594; <!-- Right Facing Caret -->
                                <span style="font-size: 10px; margin-left: 10px;">Abstract</span>
                            </span>
                            <div id="abstract2" style="display:none; margin-top: 10px;">
                                <p>
                                    Given a set of pre-trained skills performing specific aspects of rearrangement, i.e. Navigating, Picking up or Placing objects, we developed a hierarchical fine-tuning scheme that can fine-tune these policies simultaneously. Our method addresses the hand-off challenge in rearrangement where subsequent skills are not aligned (i.e. the navigation skill terminates too far from the couch). Our method demonstrates superior performance (about 13% in rearrangement success) over a static policy.
                                </p>
                            </div>
                        </td>
                    </tr>
                    
                    <!-- Third Content -->
                    <tr>
                        <td style="padding:20px;width:40%;vertical-align:middle">
                            <img src='images/RGL_teaser.gif' width="250" height="120">
                        </td>
                        <td width="60%" valign="middle">
                            <p>
                                <papertitle>RGL-NET: A Recurrent Graph Learning Framework</papertitle>
                                <br>
                                <strong>Abhinav Narayan Harish</strong>, Rajendra Nagar, 
                                <a href="https://people.iitgn.ac.in/~shanmuga/">Shanmuganathan Raman</a>
                                <br>
                                <em>WACV, 2022</em>
                                <br>
                                <a href="https://arxiv.org/abs/2107.12859">pdf</a> / 
                                <a href="https://github.com/absdnd/RGL_NET_Progressive_Part_Assembly">code</a>
                            </p>
                            <!-- Right Facing Caret for toggling abstract with "Abstract" text -->
                            <span onclick="toggleAbstract('abstract3')" style="cursor: pointer; font-size: 10px; color: black; font-weight: bold;">
                                &#8594; <!-- Right Facing Caret -->
                                <span style="font-size: 10px; margin-left: 10px;">Abstract</span>
                            </span>
                            <div id="abstract3" style="display:none; margin-top: 10px;">
                                <p>
                                    We propose an assembly framework that can assemble a shape in a canonical order by progressively gathering information. Compared to prior frameworks, our method achieves up to 10% improvement in part accuracy and 15% improvement in connectivity accuracy.
                                </p>
                            </div>
                        </td>
                    </tr>
                </tbody>
            </table>
            
            <script>
                function toggleAbstract(id) {
                    var element = document.getElementById(id);
                    var caret = element.previousElementSibling;
                    
                    // Toggle visibility of abstract content
                    if (element.style.display === "none") {
                        element.style.display = "block";
                        caret.innerHTML = "&#8594;"; // Right-facing caret when open
                    } else {
                        element.style.display = "none";
                        caret.innerHTML = "&#8594;"; // Right-facing caret when closed
                    }
                }
            </script>
            

          

         
           

            <!-- Bottom -->
            <table style="width:100%;vertical-align:center;border:0px;border-spacing:0px;padding:0px">
                <tr>
                    <td>
                        <br>
                        <!-- <p style="text-align:right;font-size:small;margin-bottom: 0">
                    <a href="https://cseweb.ucsd.edu/~kriegman/">&#10025;</a>
                    <a href="http://pages.ucsd.edu/~ztu/">&#10025;</a>
                    <a href="http://ai.stanford.edu/~ssrinath">&#10025;</a>
                    <a href="https://chenshen.xyz">&#10025;</a>
                    <a href="https://orzyt.cn">&#10025;</a>
                    <a href="http://floatingsong.com/">&#10025;</a>
                </p> -->
                        <hr style="margin-bottom:0;margin-top: 0">
                        <p style="text-align:left;font-size:10px">
                            <span style="font-size:10px;float:right">
                                Website inspired by <a href="https://jonbarron.info/" style="font-size: 10px;">Jon Barron</a>
                            </span>
                        </p>
                    </td>
                </tr>
            </table>

        </td>
    </tr>
  </table>
</body>

</html>
