<table style="width: 100%; border: 0px; border-spacing: 0px; border-collapse: separate; margin: auto;">
    <tbody>
        <tr>
            <td style="padding: 20px; width: 50%; vertical-align: middle; text-align: center;">
                <img src="images/aux_distill.png" alt="Auxiliary Task Distillation" style="width: 400px; height: auto;">
            </td>
            <td style="padding: 20px; width: 50%; vertical-align: middle;">
                <p>
                    <strong>Reinforcement Learning via Auxiliary Task Distillation</strong>
                </p>
                <p>
                    <strong>Abhinav Narayan Harish</strong>, Larry Heck, Josiah Hanna, Zsolt Kira, Andrew Szot
                </p>
                <p>
                    <em>European Conference of Computer Vision, 2024</em>
                </p>
                <p>
                    <a href="https://arxiv.org/abs/2406.17168">View Paper</a> | 
                    <a href="https://github.com/absdnd/aux_distill">View Code</a>
                </p>
            </td>
        </tr>
    </tbody>
</table>

'''

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;
margin-right:auto;margin-left:auto;">
        <tbody>

            <tr>
                <td style="padding:20px;width:40%;vertical-align:middle">
                    <img src='images/aux_distill.png' width="250" height="120"></div>
                </td>
                <td width="60%" valign="middle">
                    <p>
                        <!-- <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167865518300473"> -->
                        <papertitle>Reinforcement Learning via Auxiliary Task Distillation</papertitle>
                        </a>
                        <br>
                        <strong>Abhinav Narayan Harish</strong>, Larry Heck, Josiah Hanna, Zsolt Kira, Andrew Szot
                        <br>
                        <em>
                            European Conference of Computer Vision, 2024
                        </em>
                        <br>
                        <a href="https://arxiv.org/abs/2406.17168">pdf</a> / <a href="https://github.com/absdnd/aux_distill">code</a> 
                    </p>
                    <p></p>
                    <p>
                        We present Reinforcement Learning via Auxiliary Task Distillation (AuxDistill), a new method that enables reinforcement learning (RL) to perform long-horizon robot control problems by distilling behaviors from auxiliary RL tasks. AuxDistill achieves this by concurrently carrying out multi-task RL with auxiliary tasks, which are easier to learn and relevant to the main task. A weighted distillation loss transfers behaviors from these auxiliary tasks to solve the main task. We demonstrate that AuxDistill can learn a pixels-to-actions policy for a challenging multi-stage embodied object rearrangement task from the environment reward without demonstrations, a learning curriculum, or pre-trained skills. AuxDistill achieves 2.3x higher success than the previous state-of-the-art baseline in the Habitat Object Rearrangement benchmark and outperforms methods that use pre-trained skills and expert demonstrations.                            
                    </p>
                </td>
            </tr>

        </tbody>
    </table>
'''